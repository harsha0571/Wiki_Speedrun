{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2a7f3323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httplib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "base_url = 'https://en.wikipedia.org/'\n",
    "\n",
    "speedrun_examples_site = 'https://wikispeedruns.com/'\n",
    "\n",
    "\n",
    "\n",
    "def get_and_read_page_and_get_all_links(page_url):\n",
    "\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(page_url)\n",
    "\n",
    "    if(status.status != 200): # exit if page was not fetched\n",
    "        return -1\n",
    "\n",
    "    soup = BeautifulSoup(response , 'html.parser')\n",
    "\n",
    "    soup = soup.find(\"div\", id = \"bodyContent\") # filter only by body content to get rid of additionals sections\n",
    "\n",
    "    url_list = {}\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('href') and link.has_attr('title'):\n",
    "            url_list[link['title']] = link['href']\n",
    "\n",
    "    return url_list\n",
    "\n",
    "def read_page_and_get_all_links(page):\n",
    "\n",
    "    soup = BeautifulSoup(page , 'html.parser')\n",
    "\n",
    "    soup = soup.find(\"div\", id = \"bodyContent\") # filter only by body content to get rid of additionals sections\n",
    "\n",
    "    url_list = {}\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('href') and link.has_attr('title'):\n",
    "            url_list[link['title']] = link['href']\n",
    "\n",
    "    # print(url_list)\n",
    "\n",
    "    return url_list\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55e48113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [04:44, 3.03MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:23<00:00, 16720.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "glove = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38442cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'king' and 'dragon': 0.4227\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "word1 = \"king\"\n",
    "word2 = \"dragon\"\n",
    "cosine_similarity = torch.nn.functional.cosine_similarity(glove[word1], glove[word2], dim=0)\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}': {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eac863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in dictionary = 422978\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "with open('words.txt') as f: # txt with all english words to create a extensive vocab\n",
    "    texts = f.read()\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts([texts])\n",
    "\n",
    "print(\"Number of unique words in dictionary =\", len(tokenizer.word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "376b94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # loads spacy's english core library\n",
    "sw_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    res = []\n",
    "    for word in text:\n",
    "        if word not in sw_spacy:\n",
    "            res.append(word)\n",
    "    return res\n",
    "\n",
    "def fetch_sentence_embedding(sentence):\n",
    "    \n",
    "    words = sentence.lower().split() # convert to lower case and split into words\n",
    "    words = remove_stopwords(words)\n",
    "\n",
    "    tensor_list = []\n",
    "\n",
    "    for word in words:\n",
    "        tensor_list.append(glove[word])\n",
    "\n",
    "    if(len(tensor_list) == 0):\n",
    "        return -1\n",
    "\n",
    "    stacked_tensors = torch.stack(tensor_list)\n",
    "    sentence_vector = torch.mean(stacked_tensors , dim=0) # average out the word vectors to get sentence vector/tensor\n",
    "\n",
    "    return sentence_vector\n",
    "\n",
    "\n",
    "def compare_sentences(sentence1 , sentence2):\n",
    "    embedding_sentence1 = fetch_sentence_embedding(sentence1)\n",
    "    \n",
    "    embedding_sentence2 = fetch_sentence_embedding(sentence2)\n",
    "\n",
    "    if isinstance(embedding_sentence1, torch.Tensor) == False or isinstance(embedding_sentence2, torch.Tensor) == False:\n",
    "        return -1\n",
    "    \n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity(embedding_sentence1, embedding_sentence2, dim=0)\n",
    "    return cosine_similarity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d9437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page visited :  Warsaw\n",
      "Children%27s_Memorial_Health_Institute 0.7591485381126404\n",
      "Page visited :  Children%27s_Memorial_Health_Institute\n",
      "cycle detectd\n",
      "Category:Medical_research_institutes_in_Poland 0.810234546661377\n",
      "Page visited :  Category:Medical_research_institutes_in_Poland\n",
      "Category:Medical_research_institutes_by_country 0.8696174621582031\n",
      "Page visited :  Category:Medical_research_institutes_by_country\n",
      "Category:Medical_research_institutes 0.9207379817962646\n",
      "Page visited :  Category:Medical_research_institutes\n",
      "https://commons.wikimedia.orgCategory:Medical_research_institutes 0.9207379817962646\n",
      "Page visited :  https://commons.wikimedia.orgCategory:Medical_research_institutes\n",
      "https://en.wiktionary.orgSpecial:Search/Https://commons.wikimedia.orgCategory:Medical_research_institutes 0.9207379817962646\n",
      "Page visited :  https://en.wiktionary.orgSpecial:Search/Https://commons.wikimedia.orgCategory:Medical_research_institutes\n",
      "Why_was_the_page_I_created_deleted%3F 0.44333237409591675\n",
      "Page visited :  Why_was_the_page_I_created_deleted%3F\n",
      "User_access_levels#Autoconfirmed_users 0.5006533861160278\n",
      "Page visited :  User_access_levels#Autoconfirmed_users\n",
      "https://en.wiktionary.orgSpecial:Search/User_access_levels 0.5006533861160278\n",
      "Page visited :  https://en.wiktionary.orgSpecial:Search/User_access_levels\n",
      "Case_sensitivity 0.4178851842880249\n",
      "Page visited :  Case_sensitivity\n",
      "Database_management_system 0.6462839841842651\n",
      "Page visited :  Database_management_system\n",
      "Operations_research 0.8846338391304016\n",
      "Page visited :  Operations_research\n",
      "Talk:Operations_research 0.9999998807907104\n",
      "Page visited :  Talk:Operations_research\n",
      "Operations_research#Historical_origins 0.8846338391304016\n",
      "Page visited :  Operations_research#Historical_origins\n",
      "Research 0.9999998807907104\n",
      "Run for Research complete\n",
      "my_function() executed in 30.0202 seconds\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "# chrome_options.add_argument(\"--headless=new\") # to run it headless\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "base_url = 'https://en.wikipedia.org/wiki/'\n",
    "\n",
    "visited = {}\n",
    "\n",
    "def speed_run(start , end , timeout):\n",
    "    \n",
    "    page_url = base_url + start.replace(' ','_')\n",
    "\n",
    "    driver.get(page_url)\n",
    "\n",
    "    if(start.replace('_',' ') == end):\n",
    "        print(f\"Run for {end} complete\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        element_present = EC.presence_of_element_located(\n",
    "            (By.ID, 'bodyContent'))\n",
    "        WebDriverWait(driver, timeout).until(element_present)\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "    finally:\n",
    "        \n",
    "        visited[start] = page_url\n",
    "        \n",
    "        print(\"Page visited : \",start)\n",
    "\n",
    "        html = driver.page_source\n",
    "        links = read_page_and_get_all_links(html)\n",
    "\n",
    "        \n",
    "\n",
    "        max_score = -1\n",
    "        most_promising_link = ''\n",
    "\n",
    "        all_links = {}\n",
    "\n",
    "        \n",
    "\n",
    "        for key , value in links.items():\n",
    "            cur_score = compare_sentences(key , end)\n",
    "            all_links[key] = cur_score\n",
    "\n",
    "            if(key in visited):\n",
    "                print(\"cycle detectd\")\n",
    "            \n",
    "            if(cur_score > max_score and '/wiki/' in value and '.orgSpecial' not in value and value.replace('/wiki/Wikipedia:','').replace('/wiki/','') not in visited):\n",
    "                max_score = cur_score\n",
    "                most_promising_link = value.replace('/wiki/Wikipedia:','').replace('/wiki/','')\n",
    "\n",
    "        print(most_promising_link , max_score)\n",
    "\n",
    "        speed_run(most_promising_link , end , timeout)\n",
    "\n",
    "\n",
    "        # print(dict(sorted(all_links.items() , reverse=True)))\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "# speed_run('Bob Dylan','Painting', 5)\n",
    "# speed_run('Mona Lisa','Bob Dylan', 5)\n",
    "speed_run('Warsaw','Research',5)\n",
    "\n",
    "# speed_run('Pink salmon','Hatsune Miku',5)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Found page in {elapsed_time:.4f} seconds\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
